import os
import re
import string
import math
import numpy as np
from collections import defaultdict
from sklearn.model_selection import train_test_split


DATA_DIR = 'enron'  # 数据集地址
target_names = ['ham', 'spam']  # 正常、 垃圾 
stopwords = set(open('stopwords.txt', 'r').read().splitlines())   # .splitlines() 按换行符分割


def get_data(DATA_DIR):
    subfolders = ['enron%d' % i for i in range(1,7)]  # 返回enron1-enron7列表
    data = []
    target = []
    for subfolder in subfolders: # 对enron1-enron7依次遍历
        # spam
        spam_files = os.listdir(os.path.join(DATA_DIR, subfolder, 'spam')) # 获文件夹下的所有文件 返回一个列表
        for spam_file in spam_files: # 依次打开返回的文件
            with open(os.path.join(DATA_DIR, subfolder, 'spam', spam_file), encoding="latin-1") as f:
                data.append(f.read())
                target.append(1)  # 垃圾邮件标签 为1
        # ham
        ham_files = os.listdir(os.path.join(DATA_DIR, subfolder, 'ham'))
        for ham_file in ham_files:
            with open(os.path.join(DATA_DIR, subfolder, 'ham', ham_file), encoding="latin-1") as f:
                data.append(f.read())
                target.append(0)  # 正常邮件标签 为0
    return data, target


def preprocess(text):
    """
    对文本进行处理，包括去除标点、转为小写、去除停用词等操作。
    """
    text = text.lower()  # 转为小写
    text = re.sub(f'[{string.punctuation}]', ' ', text)  # 去除标点符号
    text = [word for word in text.split() if word not in stopwords]  # 去除停用词
    return text


class NaiveBayesClassifier():
    def __init__(self):
        self.vocabulary = set()  # 词汇表
        self.class_total = defaultdict(int)  # 每个类别的文档数
        self.word_total = defaultdict(int)  # 每个类别中所有单词出现次数之和
        self.word_given_class = defaultdict(lambda: defaultdict(int))  # 每个类别中每个单词出现次数

    def fit(self, X, y):
        """
        训练分类器，其中 X 为训练集数据，y 为训练集标签。
        """
        for text, label in zip(X, y):
            words = preprocess(text)  # 对文本进行处理，包括去除标点、转为小写、去除停用词等操作。
            self.class_total[label] += 1   # 该类别文档数加1
            for word in words:             # 遍历每一个单词
                self.vocabulary.add(word)   # 加入词汇表
                self.word_given_class[label][word] += 1  # 该类别中该单词出现次数加1
                self.word_total[label] += 1  # 该类别所有单词出现次数之和加1

    def predict(self, X):
        """
        对新邮件进行分类，其中 X 为待分类邮件。
        """
        log_priors = {}  # 存储每个类别的先验概率的对数（用于避免下溢）
        for c in self.class_total.keys():
            #  log（每个类别的文档数/文档数）  也就是训练集正常邮件和垃圾邮件占总邮件的比例
            log_priors[c] = math.log(self.class_total[c] / sum(self.class_total.values())) 

        predictions = []
        for text in X:  # 遍历测试集的每一个邮件
            words = preprocess(text) # 对文本进行处理，包括去除标点、转为小写、去除停用词等操作。

            log_probs = {}
            for c in self.class_total.keys(): #遍历两个类别
                log_probs[c] = log_priors[c]
                for word in words:  # 遍历每个单词
                    if word in self.vocabulary:  # 如果测试邮件的这个单词在对应类别词汇表中
                        # 计算条件概率的对数
                        log_probs[c] += math.log((self.word_given_class[c][word] + 1) / (self.word_total[c] + len(self.vocabulary)))  
            predictions.append(max(log_probs, key=log_probs.get))  # 取概率最大值对应的类别为预测结果

        return predictions
